{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import dask_searchcv as dcv\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "categories = [\n",
    "    'sci.electronics',\n",
    "    'sci.space',\n",
    "    'sci.med'\n",
    "]\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=5, ngram_range=(1, 2)), TfidfTransformer(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96962879640044997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_data.data, train_data.target);\n",
    "\n",
    "accuracy_score(pipeline.predict(train_data.data), train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82417582417582413"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pipeline.predict(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\"countvectorizer__min_df\":[1],\n",
    "            \"countvectorizer__ngram_range\":[[1,2]],\n",
    "            \"tfidftransformer__norm\":[\"l2\"],\n",
    "            \"logisticregression__C\":[0.1],\n",
    "            \"countvectorizer__stop_words\":[None,'english'],\n",
    "            \"countvectorizer__max_features\":[None],\n",
    "            \"countvectorizer__min_df\":[1],\n",
    "            \"countvectorizer__lowercase\":['word'],\n",
    "             \"logisticregression__penalty\":[\"l2\"]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=dcv.GridSearchCV(pipeline,param_grid,scoring='accuracy',cv=3,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 13.2s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    grid.fit(test_data.data,test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88503803888419275"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__lowercase': 'word',\n",
       " 'countvectorizer__max_features': None,\n",
       " 'countvectorizer__min_df': 1,\n",
       " 'countvectorizer__ngram_range': [1, 2],\n",
       " 'countvectorizer__stop_words': 'english',\n",
       " 'logisticregression__C': 0.1,\n",
       " 'logisticregression__penalty': 'l2',\n",
       " 'tfidftransformer__norm': 'l2'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9737954353338969"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(grid.best_estimator_.predict(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"count_vectorizer_params\": \n",
    "    {\n",
    "        \"min_df\": 5,\n",
    "        \"ngram_range\": [1, 2]\n",
    "    }, \n",
    "    \"tfidf_transformer_params\": {\n",
    "        \"norm\": \"l1\"    \n",
    "    }, \n",
    "    \"logistic_regression_params\": {\n",
    "        \"C\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854486222831\n",
      "CPU times: user 8.08 s, sys: 485 ms, total: 8.57 s\n",
      "Wall time: 7.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Чекер\n",
    "# с вашими параметрами должен отработать за 1 минуту на машинке для проверки.\n",
    "# Для сравнения на text_classification_params_example.json чекер работает 15 секунд.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import signal\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "\n",
    "# SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "SCRIPT_DIR=\"/Users/roman/DMIA/industry/hw03/\"\n",
    "\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    def __init__(self):\n",
    "        self.data = fetch_20newsgroups(\n",
    "            subset='all', \n",
    "            categories=[\n",
    "                'rec.autos',\n",
    "                'rec.motorcycles',\n",
    "                'rec.sport.baseball',\n",
    "                'rec.sport.hockey'\n",
    "            ], \n",
    "            remove=('headers', 'footers', 'quotes')\n",
    "        )\n",
    "\n",
    "    def check(self, params_path):\n",
    "        try:\n",
    "            with open(params_path, 'r') as f:\n",
    "                params = json.load(f)\n",
    "\n",
    "            signal.signal(signal.SIGALRM, signal_handler)\n",
    "            signal.alarm(60)\n",
    "            pipeline = make_pipeline(\n",
    "                CountVectorizer(**params['count_vectorizer_params']), \n",
    "                TfidfTransformer(**params['tfidf_transformer_params']), \n",
    "                LogisticRegression(**params['logistic_regression_params'])\n",
    "            )\n",
    "            score = np.mean(cross_val_score(\n",
    "                pipeline, \n",
    "                self.data.data, \n",
    "                self.data.target,\n",
    "                scoring='accuracy', \n",
    "                cv=3\n",
    "            ))\n",
    "        except:\n",
    "            traceback.print_exception(*sys.exc_info())\n",
    "            score = None\n",
    "        \n",
    "        return score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     print(Checker().check(SCRIPT_DIR + '/text_classification_params_example.json'))\n",
    "    print(Checker().check(SCRIPT_DIR + '/text_classification_params_matiiv.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import signal\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "\n",
    "# SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "SCRIPT_DIR=\"/Users/roman/DMIA/industry/hw03/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    def __init__(self):\n",
    "        self.X_data, self.y_data = make_classification(\n",
    "            n_samples=10000, n_features=20, \n",
    "            n_classes=2, n_informative=20, \n",
    "            n_redundant=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.applications = 0\n",
    "\n",
    "    def check(self, script_path):\n",
    "        try:\n",
    "            signal.signal(signal.SIGALRM, signal_handler)\n",
    "            signal.alarm(20)\n",
    "            algo_impl = imp.load_source('logistic_regression_{}'.format(self.applications), script_path)\n",
    "            self.applications += 1\n",
    "            algo = algo_impl.MyLogisticRegression(**algo_impl.LR_PARAMS_DICT)\n",
    "            return np.mean(cross_val_score(algo, self.X_data, self.y_data, cv=2, scoring='accuracy'))\n",
    "        except:\n",
    "            traceback.print_exception(*sys.exc_info())\n",
    "            return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(Checker().check(SCRIPT_DIR + '/logistic_regression_example.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import signal\n",
    "import traceback\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "# SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "SCRIPT_DIR=\"/Users/roman/DMIA/industry/hw03/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    def __init__(self):\n",
    "        # ВНИМАНИЕ !!!\n",
    "        # При тестировании seed будет изменён\n",
    "        # Не переобучитесь!\n",
    "        random_gen = np.random.RandomState(42)\n",
    "        \n",
    "        weights = (0.05 + random_gen.exponential(0.75, size=15)) * 2\n",
    "        X_data = random_gen.uniform(0., 4, size=(40, 15))\n",
    "        errors = random_gen.normal(0., 2., size=40)\n",
    "\n",
    "        split_pos = 25\n",
    "        self.X_train = X_data[:split_pos]\n",
    "        self.errors_train = errors[:split_pos]\n",
    "        self.X_test = X_data[split_pos:]\n",
    "        self.errors_test = errors[split_pos:]\n",
    "        self.weights = weights\n",
    "\n",
    "        self.applications = 0\n",
    "\n",
    "    def check(self, script_path):\n",
    "        try:\n",
    "            signal.signal(signal.SIGALRM, signal_handler)\n",
    "            signal.alarm(120)\n",
    "            algo_impl = imp.load_source('algo_impl_{}'.format(self.applications), script_path)\n",
    "            self.applications += 1\n",
    "            algo = algo_impl.Optimizer()\n",
    "            algo.fit(np.array(self.X_train), np.dot(self.X_train, self.weights) + self.errors_train)\n",
    "            \n",
    "            saved_moneys = 0.\n",
    "            for budget, target_error in zip(self.X_test, self.errors_test):\n",
    "                origin_budget = np.array(budget)\n",
    "                optimized_budget = np.array(algo.optimize(origin_budget))\n",
    "\n",
    "                if ((origin_budget * 0.95 <= optimized_budget) & (optimized_budget <= origin_budget * 1.05)).all():\n",
    "                    if np.dot(optimized_budget, self.weights) >=  np.dot(origin_budget, self.weights):\n",
    "                        saved_moneys += np.sum(origin_budget) - np.sum(optimized_budget)\n",
    "\n",
    "            return saved_moneys\n",
    "        except:\n",
    "            traceback.print_exception(*sys.exc_info())\n",
    "            return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(Checker().check(SCRIPT_DIR + '/ad_budget_example.py'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
