{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import dask_searchcv as dcv\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "categories = [\n",
    "    'sci.electronics',\n",
    "    'sci.space',\n",
    "    'sci.med'\n",
    "]\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=5, ngram_range=(1, 2)), TfidfTransformer(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96962879640044997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(train_data.data, train_data.target);\n",
    "\n",
    "accuracy_score(pipeline.predict(train_data.data), train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82417582417582413"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pipeline.predict(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={\"countvectorizer__min_df\":[1],\n",
    "            \"countvectorizer__ngram_range\":[[1,2]],\n",
    "            \"tfidftransformer__norm\":[\"l2\"],\n",
    "            \"logisticregression__C\":[0.1],\n",
    "            \"countvectorizer__stop_words\":[None,'english'],\n",
    "            \"countvectorizer__max_features\":[None],\n",
    "            \"countvectorizer__min_df\":[1],\n",
    "            \"countvectorizer__lowercase\":['word'],\n",
    "             \"logisticregression__penalty\":[\"l2\"]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=dcv.GridSearchCV(pipeline,param_grid,scoring='accuracy',cv=3,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 13.2s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    grid.fit(test_data.data,test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88503803888419275"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__lowercase': 'word',\n",
       " 'countvectorizer__max_features': None,\n",
       " 'countvectorizer__min_df': 1,\n",
       " 'countvectorizer__ngram_range': [1, 2],\n",
       " 'countvectorizer__stop_words': 'english',\n",
       " 'logisticregression__C': 0.1,\n",
       " 'logisticregression__penalty': 'l2',\n",
       " 'tfidftransformer__norm': 'l2'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9737954353338969"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(grid.best_estimator_.predict(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"count_vectorizer_params\": \n",
    "    {\n",
    "        \"min_df\": 5,\n",
    "        \"ngram_range\": [1, 2]\n",
    "    }, \n",
    "    \"tfidf_transformer_params\": {\n",
    "        \"norm\": \"l1\"    \n",
    "    }, \n",
    "    \"logistic_regression_params\": {\n",
    "        \"C\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.854486222831\n",
      "CPU times: user 8.21 s, sys: 254 ms, total: 8.47 s\n",
      "Wall time: 7.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Чекер\n",
    "# с вашими параметрами должен отработать за 1 минуту на машинке для проверки.\n",
    "# Для сравнения на text_classification_params_example.json чекер работает 15 секунд.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import signal\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "\n",
    "# SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "SCRIPT_DIR=\"/Users/roman/DMIA/industry/hw03/\"\n",
    "\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    def __init__(self):\n",
    "        self.data = fetch_20newsgroups(\n",
    "            subset='all', \n",
    "            categories=[\n",
    "                'rec.autos',\n",
    "                'rec.motorcycles',\n",
    "                'rec.sport.baseball',\n",
    "                'rec.sport.hockey'\n",
    "            ], \n",
    "            remove=('headers', 'footers', 'quotes')\n",
    "        )\n",
    "\n",
    "    def check(self, params_path):\n",
    "        try:\n",
    "            with open(params_path, 'r') as f:\n",
    "                params = json.load(f)\n",
    "\n",
    "            signal.signal(signal.SIGALRM, signal_handler)\n",
    "            signal.alarm(60)\n",
    "            pipeline = make_pipeline(\n",
    "                CountVectorizer(**params['count_vectorizer_params']), \n",
    "                TfidfTransformer(**params['tfidf_transformer_params']), \n",
    "                LogisticRegression(**params['logistic_regression_params'])\n",
    "            )\n",
    "            score = np.mean(cross_val_score(\n",
    "                pipeline, \n",
    "                self.data.data, \n",
    "                self.data.target,\n",
    "                scoring='accuracy', \n",
    "                cv=3\n",
    "            ))\n",
    "        except:\n",
    "            traceback.print_exception(*sys.exc_info())\n",
    "            score = None\n",
    "        \n",
    "        return score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     print(Checker().check(SCRIPT_DIR + '/text_classification_params_example.json'))\n",
    "    print(Checker().check(SCRIPT_DIR + '/text_classification_params_matiiv.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "X_data, y_data = make_classification(\n",
    "            n_samples=10000, n_features=20, \n",
    "            n_classes=2, n_informative=20, \n",
    "            n_redundant=0,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "size, dim = X_data.shape\n",
    "\n",
    "random_gen = np.random.RandomState(777)\n",
    "w = random_gen.rand(dim)\n",
    "w0 = random_gen.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021156520492997814"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1+np.exp(np.dot(w,X_data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1410.75174871\n",
      "529.994857836\n",
      "294.940626138\n",
      "-1004.37684016\n",
      "88.8285496361\n",
      "271.269035961\n",
      "886.945728685\n",
      "-515.747785874\n",
      "-121.316550995\n",
      "-563.385556806\n",
      "88.1041489549\n",
      "542.690421005\n",
      "-503.034997813\n",
      "-464.841847945\n",
      "608.475646674\n",
      "648.502572684\n",
      "-635.588257107\n",
      "-760.684508686\n",
      "-441.949373465\n",
      "-39.0141730084\n"
     ]
    }
   ],
   "source": [
    "for i in X_data[0]:\n",
    "    print(np.sum(y_data-1/(1+np.exp(np.sum(X_data*w,axis=1))))*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1410.7517487083196,\n",
       " 529.99485783627438,\n",
       " 294.94062613847609,\n",
       " -1004.3768401646312,\n",
       " 88.828549636126809,\n",
       " 271.26903596055411,\n",
       " 886.94572868515763,\n",
       " -515.74778587418291,\n",
       " -121.31655099507678,\n",
       " -563.38555680636284,\n",
       " 88.104148954863959,\n",
       " 542.69042100522711,\n",
       " -503.03499781269386,\n",
       " -464.84184794546883,\n",
       " 608.47564667385939,\n",
       " 648.50257268422104,\n",
       " -635.58825710742838,\n",
       " -760.68450868630907,\n",
       " -441.94937346500558,\n",
       " -39.014173008365233]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.sum(y_data-1/(1+np.exp(np.sum(X_data*w,axis=1))))*i for i in X_data[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roman/DMIA/industry/hw03//logistic_regression_likehood.py:51: RuntimeWarning: overflow encountered in exp\n",
      "  ders_w+=[np.sum(-y+1/(1+np.exp(np.sum(x*self.w,axis=1))))*i for i in x_i] # можно np.mean\n",
      "/Users/roman/DMIA/industry/hw03//logistic_regression_likehood.py:51: RuntimeWarning: overflow encountered in exp\n",
      "  ders_w+=[np.sum(-y+1/(1+np.exp(np.sum(x*self.w,axis=1))))*i for i in x_i] # можно np.mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4289\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import signal\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "\n",
    "# SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "SCRIPT_DIR=\"/Users/roman/DMIA/industry/hw03/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    def __init__(self):\n",
    "        self.X_data, self.y_data = make_classification(\n",
    "            n_samples=10000, n_features=20, \n",
    "            n_classes=2, n_informative=20, \n",
    "            n_redundant=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.applications = 0\n",
    "\n",
    "    def check(self, script_path):\n",
    "        try:\n",
    "            signal.signal(signal.SIGALRM, signal_handler)\n",
    "            signal.alarm(200)\n",
    "            \n",
    "            algo_impl = imp.load_source('logistic_regression_{}'.format(self.applications), script_path)\n",
    "            self.applications += 1\n",
    "            algo = algo_impl.MyLogisticRegression(**algo_impl.LR_PARAMS_DICT)\n",
    "            return np.mean(cross_val_score(algo, self.X_data, self.y_data, cv=2, scoring='accuracy'))\n",
    "        except:\n",
    "            traceback.print_exception(*sys.exc_info())\n",
    "            return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     print(Checker().check(SCRIPT_DIR + '/logistic_regression_example.py'))\n",
    "#      print(Checker().check(SCRIPT_DIR + '/logistic_regression_matiiv.py'))\n",
    "#      print(Checker().check(SCRIPT_DIR + '/logistic_regression_matiiv2.py'))\n",
    "     print(Checker().check(SCRIPT_DIR + '/logistic_regression_likehood.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.50448887039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "import imp\n",
    "import signal\n",
    "import traceback\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "# SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "SCRIPT_DIR=\"/Users/roman/DMIA/industry/hw03/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    def __init__(self):\n",
    "        # ВНИМАНИЕ !!!\n",
    "        # При тестировании seed будет изменён\n",
    "        # Не переобучитесь!\n",
    "        random_gen = np.random.RandomState(42)\n",
    "        \n",
    "        weights = (0.05 + random_gen.exponential(0.75, size=15)) * 2\n",
    "        X_data = random_gen.uniform(0., 4, size=(40, 15))\n",
    "        errors = random_gen.normal(0., 2., size=40)\n",
    "\n",
    "        split_pos = 25\n",
    "        self.X_train = X_data[:split_pos]\n",
    "        self.errors_train = errors[:split_pos]\n",
    "        self.X_test = X_data[split_pos:]\n",
    "        self.errors_test = errors[split_pos:]\n",
    "        self.weights = weights\n",
    "\n",
    "        self.applications = 0\n",
    "\n",
    "    def check(self, script_path):\n",
    "        try:\n",
    "            signal.signal(signal.SIGALRM, signal_handler)\n",
    "            signal.alarm(120)\n",
    "            algo_impl = imp.load_source('algo_impl_{}'.format(self.applications), script_path)\n",
    "            self.applications += 1\n",
    "            algo = algo_impl.Optimizer()\n",
    "            algo.fit(np.array(self.X_train), np.dot(self.X_train, self.weights) + self.errors_train)\n",
    "            \n",
    "            saved_moneys = 0.\n",
    "            for budget, target_error in zip(self.X_test, self.errors_test):\n",
    "                origin_budget = np.array(budget)\n",
    "                optimized_budget = np.array(algo.optimize(origin_budget))\n",
    "\n",
    "                if ((origin_budget * 0.95 <= optimized_budget) & (optimized_budget <= origin_budget * 1.05)).all():\n",
    "                    if np.dot(optimized_budget, self.weights) >=  np.dot(origin_budget, self.weights):\n",
    "                        saved_moneys += np.sum(origin_budget) - np.sum(optimized_budget)\n",
    "\n",
    "            return saved_moneys\n",
    "        except:\n",
    "            traceback.prin\n",
    "            t_exception(*sys.exc_info())\n",
    "            return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     print(Checker().check(SCRIPT_DIR + '/ad_budget_example.py'))\n",
    "    print(Checker().check(SCRIPT_DIR + '/ad_budget_matiiv.py'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
